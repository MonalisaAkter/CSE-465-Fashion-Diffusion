# -*- coding: utf-8 -*-
"""Fashion Diffusion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jb3rfBsVuJKOl-dLKQ7u-HNRbJb1txjB
"""

from google.colab import drive
drive.mount('/content/gdrive')

#@markdown Check type of GPU and VRAM available.
!nvidia-smi --query-gpu=name,memory.total,memory.free --format=csv,noheader

import PIL
import os

# Commented out IPython magic to ensure Python compatibility.
#@title Install Requirements ðŸ¤—
!wget -q https://github.com/ShivamShrirao/diffusers/raw/9db565e15e8e35f1ebb48e1409b8ddb129f6f1e9/examples/dreambooth/train_dreambooth.py
# %pip install -qq git+https://github.com/ShivamShrirao/diffusers
# %pip install -q -U --pre triton
# %pip install -q accelerate>=0.20.3 transformers ftfy bitsandbytes gradio

#@title Login to HuggingFace
from huggingface_hub import notebook_login
!git config --global credential.helper store
notebook_login()

# Commented out IPython magic to ensure Python compatibility.
#@title Install xformers from precompiled wheel.
# !pip install --no-deps -q "https://github.com/metrolobo/xformers_wheels/releases/download/4c06c79_various6/xformers-0.0.15.dev0_4c06c79.d20221201-cp38-cp38-linux_x86_64.whl"
# These were compiled on Tesla T4, should also work on P100, thanks to https://github.com/metrolobo

# If precompiled wheels don't work, install it with the following command. It will take around 40 minutes to compile.
# %pip install "git+https://github.com/facebookresearch/xformers@1d31a3a#egg=xformers"

#@markdown Name/Path of the initial model.
MODEL_NAME = "stabilityai/stable-diffusion-2" #@param {type:"string"}

#@markdown Path for images of the concept for training.
INSTANCE_DIR = "/content/data/sks" #@param {type:"string"}
!mkdir -p $INSTANCE_DIR

#@markdown A general name for class like dog for dog images.
CLASS_NAME = "kurta" #@param {type:"string"}
CLASS_DIR = f"/content/data/{CLASS_NAME}"

#@markdown A rare token name which will reference the subject.
TOKEN_NAME = "BD" #@param {type:"string"}

#@markdown If model weights should be saved directly in google drive (takes around 4-5 GB).
save_to_gdrive = False #@param {type:"boolean"}
if save_to_gdrive:
    from google.colab import drive
    drive.mount('/content/drive')

#@markdown Enter the directory name to save model at.

OUTPUT_DIR = "stable_diffusion_weights/sks" #@param {type:"string"}
if save_to_gdrive:
    OUTPUT_DIR = "/content/gdrive/MyDrive/465_project/checkpoints/" + OUTPUT_DIR
else:
    OUTPUT_DIR = "/content/" + OUTPUT_DIR

print(f"[*] Weights will be saved at {OUTPUT_DIR}")

!mkdir -p $OUTPUT_DIR

#@markdown sks is a rare identifier, feel free to replace it.

for img in os.listdir('/content/data/kurta_images'):
  image = PIL.Image.open(os.path.join('/content/data/kurta_images',img)).convert('RGBA').resize((512,512))
  image.save(os.path.join('/content/data/sks', img.replace('jpg', 'png')))

!accelerate launch train_dreambooth.py \
  --pretrained_model_name_or_path=$MODEL_NAME \
  --instance_data_dir=$INSTANCE_DIR \
  --class_data_dir=$CLASS_DIR \
  --output_dir=$OUTPUT_DIR \
  --with_prior_preservation --prior_loss_weight=1.0 \
  --instance_prompt="photo of {TOKEN_NAME} {CLASS_NAME}" \
  --class_prompt="photo of a {CLASS_NAME}" \
  --seed=1337 \
  --resolution=512 \
  --train_batch_size=1 \
  --mixed_precision="fp16" \
  --use_8bit_adam \
  --gradient_accumulation_steps=2 \
  --learning_rate=5e-6 \
  --lr_scheduler="constant" \
  --lr_warmup_steps=0 \
  --num_class_images=50 \
  --sample_batch_size=4 \
  --max_train_steps=1000

!wget -q https://github.com/ShivamShrirao/diffusers/raw/9db565e15e8e35f1ebb48e1409b8ddb129f6f1e9/scripts/convert_diffusers_to_original_stable_diffusion.py

ckpt_path = OUTPUT_DIR + "/model.ckpt"

half_arg = ""
#@markdown  Whether to convert to fp16, takes half the space (2GB), might loose some quality.
fp16 = False #@param {type: "boolean"}
if fp16:
    half_arg = "--half"
!python convert_diffusers_to_original_stable_diffusion.py --model_path $OUTPUT_DIR  --checkpoint_path $ckpt_path $half_arg
print(f"[*] Converted ckpt saved at {ckpt_path}")

import torch
from torch import autocast
from diffusers import StableDiffusionPipeline, StableDiffusionInpaintPipeline
from IPython.display import display

model_path = "runwayml/stable-diffusion-inpainting"             # If you want to use previously trained model saved in gdrive, replace this with the full path of model in gdrive

# pipe = StableDiffusionPipeline.from_pretrained(model_path, torch_dtype=torch.float16).to("cuda")
pipe = StableDiffusionInpaintPipeline.from_pretrained(model_path,torch_dtype=torch.float16).to("cuda")
g_cuda = None

# Commented out IPython magic to ensure Python compatibility.
import PIL
import gradio as gr
import numpy as np

def image_grid(imgs, rows, cols):
    assert len(imgs) == rows*cols

    w, h = imgs[0].size
    grid = PIL.Image.new('RGB', size=(cols*w, rows*h))
    grid_w, grid_h = grid.size

    for i, img in enumerate(imgs):
        grid.paste(img, box=(i%cols*w, i//cols*h))
    return grid

!pip install iglovikov_helper_functions
!wget https://habrastorage.org/webt/em/l7/cr/eml7crxnxftrimsmolwjegqcrp4.jpeg > /dev/null
!pip install cloths_segmentation  > /dev/null

# %matplotlib inline
from pylab import imshow
import cv2
import albumentations as albu

from iglovikov_helper_functions.utils.image_utils import load_rgb, pad, unpad
from iglovikov_helper_functions.dl.pytorch.utils import tensor_from_rgb_image

from cloths_segmentation.pre_trained_models import create_model
model = create_model("Unet_2020-10-30")



model.eval();
image = PIL.Image.open("/content/test_img/0018.jpg").convert('RGB').resize((512,512))
open_cv_image = np.array(image)
imshow(open_cv_image)
image.save("002_resized.jpg", "JPEG")

transform = albu.Compose([albu.Normalize(p=1)], p=1)
padded_image, pads = pad(open_cv_image, factor=32, border=cv2.BORDER_CONSTANT)
x = transform(image=padded_image)["image"]
x = torch.unsqueeze(tensor_from_rgb_image(x), 0)
with torch.no_grad():
  prediction = model(x)[0][0]

mask = (prediction > 0).cpu().numpy().astype(np.uint8)
mask = unpad(mask, pads)
mask_img = cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB) * 255
mask_img = mask_img
im_pil = PIL.Image.fromarray(mask_img)
imshow(im_pil)
im_pil.save("002_mask.jpg", "JPEG")

prompt = "a man wearing BD kurta" #@param {type:"string"}
negative_prompt = "" #@param {type:"string"}
num_samples = 4 #@param {type:"number"}
guidance_scale = 7.5 #@param {type:"number"}
num_inference_steps = 200 #@param {type:"number"}
height = 512 #@param {type:"number"}
width = 512 #@param {type:"number"}

generator = torch.Generator(device="cuda").manual_seed(1) # change the seed to get different results

images = pipe(
    prompt=prompt,
    image=image,
    mask_image=im_pil,
    guidance_scale=guidance_scale,
    generator=generator,
    num_images_per_prompt=num_samples,
    num_inference_steps=num_inference_steps
).images

# insert initial image in
# the list so we can compare side by side
images.insert(0, image)
image_grid(images, 1, num_samples)

